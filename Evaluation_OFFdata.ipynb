{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR and NLP Model Evaluation \n",
    "In this notebook, we will evaluate six models for the OCR and NLP components of Gredient. We will use data from the public dataset [Open Food Facts](https://world.openfoodfacts.org/data), that contains images and annotations of product ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNOTATE AND CHECK FOR CONSISTENCY AND CODE MAKES SENSE -- VERIFY THAT SCORES AND GRAPHS ARE ACCURATE/ WRITE CODE TO SIMPLIFY REDUNDANCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import base64\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for connecting to S3\n",
    "import boto3 \n",
    "import botocore \n",
    "from sagemaker import get_execution_role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1a8f302e162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import libraries for preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# import libraries for preprocessing\n",
    "import urllib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for OCR \n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for postprocessing\n",
    "import chars2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to S3 and read data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OFF data in S3\n",
    "role = get_execution_role() \n",
    "bucket = 'sagemaker-060720' \n",
    "data_key = 'evalOFFdata.csv' \n",
    "data_location = 's3://{}/{}'.format(bucket, data_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OFF data\n",
    "eval_data = pd.read_csv(data_location)\n",
    "print(eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess for OCR..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample N data for evaluation\n",
    "n=1000\n",
    "data = eval_data.sample(n, random_state=210).reset_index()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert url to images\n",
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image urls to np arrays with RGB\n",
    "imgs = [url_to_image(image_url) for image_url in data.image_ingredients_url]\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In previous iterations, we implemented and evaluated five image preprocessing methods that included combinations of grayscaling, denoising, thresholding, dilating, eroding, opening, deskewing, and canny edge detection. We then evaluated the performance of the OCR models with these preprocessed images. The performance decreased significantly, so we decided that the preprocessing methods built into the models were sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Evaluation\n",
    "\n",
    "In this section, we will evaluate Pytesseract, Amazon Rekognition, and Amazon Textract on their ability to correctly detect ingredients from an image. We use the OFF data to first run each model on the whole sample of data, which includes both high and low quality images. At that point, performance is fairly low over all models. So to proxy performance on high quality images, we get the top 210 scoring detections from each model and run all three models on each set of top 210 images. We average the 630 F1 scores for each model to get our final accuracy metric used for evaluation. Additionally, we record the time in seconds for each model to run and include speed in our evaluation as well. Since the performance of detections is highly dependent on image quality, we encourage our users through the interface to take better quality photos by providing them a cropping mechanism and messages indicating what a good and bad quality image looks like. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean word tokens\n",
    "def clean_word(word):\n",
    "    \n",
    "    c_word = word.lower().strip() # lowercase and remove white space\n",
    "    c_word = re.sub('[^a-zA-Z]+', '', c_word) # remove anything that's not a letter\n",
    "    if len(c_word) < 2: # remove words that are less than 2 characters\n",
    "        c_word = \"\" \n",
    "    \n",
    "    return c_word\n",
    "\n",
    "# clean list of strings \n",
    "def clean_text(text, split=True):\n",
    "    \n",
    "    if split == False: # for ocr output\n",
    "        c_text = [clean_word(w) for w in text] # already split and clean words\n",
    "        \n",
    "    else: \n",
    "        c_text = re.sub('[0-9]', ' ', text) # replace numbers with space \n",
    "        c_text = re.sub('['+string.punctuation+']', ' ', c_text) # replace punctuation with space\n",
    "        c_text = [clean_word(w) for w in c_text.split()] # split on spaces and clean words\n",
    "      \n",
    "    c_text = sorted(list(filter(None, set(c_text)))) # remove empty words and get unique values and sort\n",
    "    \n",
    "    return c_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision: #(detected words that are in ingredients) / #(all detected words)\n",
    "def precision(ing_lst, i, ingredients):\n",
    "    detected_words = ing_lst[i]\n",
    "    actual_ingredients = ingredients[i]\n",
    "    if len(detected_words) == 0:\n",
    "        return 0 # 0 if no detected words \n",
    "    else:\n",
    "        tp = sum([dw in actual_ingredients for dw in detected_words])\n",
    "        p = len(detected_words) # tp+fp (positives)\n",
    "        return tp/p\n",
    "    \n",
    "    \n",
    "# recall: #(detected words that are in ingredients) / #(all words in ingredients)\n",
    "def recall(ing_lst, i, ingredients):\n",
    "    detected_words = ing_lst[i]\n",
    "    actual_ingredients = ingredients[i]\n",
    "    if len(detected_words) == 0:\n",
    "        return 0 # 0 if no detected words \n",
    "    else:\n",
    "        tp = sum([dw in actual_ingredients for dw in detected_words])\n",
    "        a = len(actual_ingredients) # tp+fn (actual)\n",
    "        return tp/a\n",
    "    \n",
    "\n",
    "# f1 score: 2*precision*recall / precision+recall\n",
    "def F1score(ing_lst, i, ingredients):\n",
    "    p = precision(ing_lst, i, ingredients)\n",
    "    r = recall(ing_lst, i, ingredients)\n",
    "    if p==0 and r==0:\n",
    "        return 0\n",
    "    return (2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top images, ingredients, and F1 score \n",
    "def top210(scores):\n",
    "    \n",
    "    s = np.array([scores])\n",
    "    inds = (-s).argsort()[0][:210] # top 210 detections\n",
    "\n",
    "    imgs210 = [imgs[i] for i in list(inds)] # images for top 210 detections \n",
    "    print(len(imgs210))\n",
    "\n",
    "    a_ings = [ingredients[i] for i in list(inds)] # actual ingredients for top 210 detections \n",
    "    print(len(a_ings))\n",
    "\n",
    "    print(\"Average top F1-score:\", sum([scores[i] for i in list(inds)])/210) # average F1 score for top 210 detections \n",
    "    \n",
    "    return [imgs210, a_ings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned List of Actual Ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients = [clean_text(ing) for ing in data.ingredients_text] # words in ingredients \n",
    "len(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pytesseract on an image\n",
    "def pytess(img) :\n",
    "    custom_oem_psm_config = r'--dpi 300 --psm 6'\n",
    "    box = pytesseract.image_to_data(img, output_type=Output.DICT, lang='eng', config=custom_oem_psm_config)\n",
    "    return box['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pytesseract to images and return time\n",
    "start_time = time.time()\n",
    "pyt_texts = [pytess(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 1.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "pyt_ingredients = [clean_text(text, split=False) for text in pyt_texts]\n",
    "len(pyt_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of tesseract\n",
    "pyt_output = pd.DataFrame({'detected':pyt_ingredients})\n",
    "pyt_output.to_csv('pyt_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data in csv\n",
    "pyt_data = pd.read_csv('pyt_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "pyt_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in pyt_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(pyt_ingredients[:5])\n",
    "print(len(pyt_ingredients))\n",
    "\n",
    "# get F1 scores for pytesseract detections\n",
    "pyt_scores = [F1score(pyt_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(pyt_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 210 images and actual ingredients\n",
    "pyt_imgs, a_pyt_ings = top210(pyt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate rekognition object\n",
    "rek_client=boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run rekognition on an image\n",
    "def rekogn(img):\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"JPEG\")\n",
    "    img_bytes = buff.getvalue()\n",
    "    rek_text = rek_client.detect_text(Image={\"Bytes\":img_bytes})\n",
    "    return rek_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rekognition to images and return time\n",
    "start_time = time.time()\n",
    "rek_texts = [rekogn(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 4.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "rek_words = [[text['DetectedText'] if text['Type']=='WORD' else \"\" for text in texts['TextDetections']] for texts in rek_texts]\n",
    "\n",
    "rek_ingredients = [clean_text(text, split=False) for text in rek_words] # detected words\n",
    "\n",
    "len(rek_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of rekognition\n",
    "rek_output = pd.DataFrame({'detected':rek_ingredients})\n",
    "rek_output.to_csv('rek_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data in csv\n",
    "rek_data = pd.read_csv('rek_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "rek_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in rek_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(rek_ingredients[:5])\n",
    "print(len(rek_ingredients))\n",
    "\n",
    "# get F1 scores for rekognition detections\n",
    "rek_scores = [F1score(rek_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(rek_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 210 images and actual ingredients\n",
    "rek_imgs, a_rek_ings = top210(rek_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate textract object\n",
    "tex_client = boto3.client('textract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run textract on an image\n",
    "def textract(img):\n",
    "    pil_img = Image.fromarray(img)\n",
    "    buff = BytesIO()\n",
    "    pil_img.save(buff, format=\"JPEG\")\n",
    "    img_bytes = buff.getvalue()\n",
    "    tex_text = tex_client.detect_document_text(Document={\"Bytes\":img_bytes})\n",
    "    return tex_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--skip if already run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply textract to images and return time\n",
    "start_time = time.time()\n",
    "tex_texts = [textract(img) for img in imgs]\n",
    "print(\"--- %s seconds ---\" % (int(time.time() - start_time)/n)) # 1.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detected words\n",
    "tex_words = [[text['Text'] if text['BlockType']=='WORD' else \"\" for text in texts['Blocks']] for texts in tex_texts]\n",
    "\n",
    "tex_ingredients = [clean_text(text, split=False) for text in tex_words] # detected words\n",
    "\n",
    "len(tex_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output of textract\n",
    "tex_output = pd.DataFrame({'detected':tex_ingredients})\n",
    "tex_output.to_csv('tex_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**--**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved data in csv\n",
    "tex_data = pd.read_csv('tex_output.csv')\n",
    "\n",
    "# get detected ingredients\n",
    "tex_ingredients = [[re.sub('[^a-zA-Z]+', '', e) for e in l.split(\",\")] for l in tex_data.detected]\n",
    "\n",
    "# peak at detected ingredients\n",
    "print(tex_ingredients[:5])\n",
    "print(len(tex_ingredients))\n",
    "\n",
    "# get F1 scores for textract detections\n",
    "tex_scores = [F1score(tex_ingredients, i, ingredients) for i in range(n)]\n",
    "\n",
    "print(\"Average F1-score:\", sum(tex_scores)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 210 images and actual ingredients\n",
    "tex_imgs, a_tex_ings = top210(tex_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Quality Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run each model on top images\n",
    "def run_all(imgs, n):\n",
    "    \n",
    "    # apply pytesseract to images and return time\n",
    "    start_time = time.time()\n",
    "    p_texts = [pytess(img) for img in imgs]\n",
    "    print(\"--- pyt %s seconds ---\" % (int(time.time() - start_time)/n))\n",
    "    p_ingredients = [clean_text(text, split=False) for text in p_texts]\n",
    "    print(len(p_ingredients))\n",
    "    \n",
    "    # apply rekognition to images and return time\n",
    "    start_time = time.time()\n",
    "    r_texts = [rekogn(img) for img in imgs]\n",
    "    print(\"--- rek %s seconds ---\" % (int(time.time() - start_time)/n)) \n",
    "    r_words = [[text['DetectedText'] if text['Type']=='WORD' else \"\" for text in texts['TextDetections']] for texts in r_texts]\n",
    "    r_ingredients = [clean_text(text, split=False) for text in r_words] # detected words\n",
    "    print(len(r_ingredients))\n",
    "    \n",
    "    # apply rekognition to images and return time\n",
    "    start_time = time.time()\n",
    "    t_texts = [textract(img) for img in imgs]\n",
    "    print(\"--- tex %s seconds ---\" % (int(time.time() - start_time)/n))\n",
    "    t_words = [[text['Text'] if text['BlockType']=='WORD' else \"\" for text in texts['Blocks']] for texts in t_texts]\n",
    "    t_ingredients = [clean_text(text, split=False) for text in t_words] # detected words\n",
    "    print(len(t_ingredients))\n",
    "    \n",
    "    return [p_ingredients, r_ingredients, t_ingredients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top scores for each model on a set of top detections\n",
    "def top_scores(top_detections, top_ingredients):\n",
    "    \n",
    "    top_s = [[F1score(td, i, ti) for i in range(210)] for td,ti in zip(top_detections,[top_ingredients]*3)]\n",
    "\n",
    "    print(\"Average F1-scores (pyt):\", sum(top_s[0])/210)\n",
    "    print(\"Average F1-scores (rek):\", sum(top_s[1])/210)\n",
    "    print(\"Average F1-scores (tex):\", sum(top_s[2])/210)\n",
    "    \n",
    "    return top_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Pytesseract Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pyt_ingredients = run_all(pyt_imgs, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pyt_scores = top_scores(top_pyt_ingredients,a_pyt_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Rekognition Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rek_ingredients = run_all(rek_imgs, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rek_scores = top_scores(top_rek_ingredients,a_rek_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Top Textract Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tex_ingredients = run_all(tex_imgs, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tex_scores = top_scores(top_tex_ingredients,a_tex_ings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Scores for All Top Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['Pytesseract', 'Rekognition', 'Textract']\n",
    "\n",
    "for i in range(3):\n",
    "    print(model[i], (sum(top_pyt_scores[i]) + sum(top_rek_scores[i]) + sum(top_tex_scores[i])) / (3*210))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Pytesseract has an average F1 score of 0.728 and speed of 1.01 seconds/image\n",
    "    - 0.84881 for top 210 \n",
    "    - 0.31855 for full sample\n",
    "- Rekognition has an average F1 score of 0.878 and speed of 4.67 seconds/image  \n",
    "    - 0.92443 for top 210 \n",
    "    - 0.50114 for full sample \n",
    "- Textract has an average F1 score of 0.868 and speed of 1.35 seconds/image  \n",
    "    - 0.91978 for top 210 \n",
    "    - 0.49600 for full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Evaluation \n",
    "\n",
    "Since Textract does well on time and accuracy, we will move forward with evaluating NLP models on both the complete sample of Textract ingredient detections and the top scoring Textract detections. We have 5 sets of allergens that will be used to set the safety of each of the products. For each set, we will evaluate three models to see which one performs the best with respect to various metrics. Specifically, a true positive is when a model correctly signals a product to be safe, since there are no ingredient and allergen matches. On the other hand, a false positive is when a model signals a product to be safe when it is actually unsafe.   \n",
    "\n",
    "The three models are (1) a perfect matching mechanism, (2) cosine similarity with CountVectorizer character embeddings, and (3) cosine similarity with Chars2Vec character embeddings. For the latter two, we tune the threshold of the cosine similarity scores to observe which setting would result in an acceptable balance between the true positive rate and the false positive rate. When the threshold is higher, the model makes fewer matches between allergens and ingredients, and thus has an increased false positive rate. Basic match represents the accuracy metrics at the highest threshold, where the cosine similarity score equals 1. We aim to find the model and the threshold that would strike the best balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the safety of ingredients for a given similarity score \n",
    "def dect_safety(safety_func, sim):  \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    top_safety = [[safety_func(person, ing, sim) for ing in top_tex_ings] for person in allergens] # detected safety for top ingredients\n",
    "    all_safety = [[safety_func(person, ing, sim) for ing in tex_ingredients] for person in allergens] # detected safety for all ingredients\n",
    "    \n",
    "    t_time = (time.time() - start_time)/(1210*5)\n",
    "    print(\"--- %s seconds ---\" % t_time)\n",
    "    \n",
    "    return [top_safety, all_safety, t_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive: safe ; negative: unsafe\n",
    "# true positives: #(classified safe products that are actually safe) \n",
    "# false positives: #(classified safe products that are actually unsafe) <- NOT GOOD \n",
    "# true negatives: #(classified unsafe products that are actually unsafe)\n",
    "# false negatives: #(classified unsafe products that are actually safe)\n",
    "\n",
    "def pred_stats(pred_safety, true_safety):\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(len(pred_safety)):\n",
    "        if (pred_safety[i]==\"safe\") & (true_safety[i]==\"safe\"):\n",
    "            tp += 1\n",
    "        if (pred_safety[i]==\"safe\") & (true_safety[i]==\"unsafe\"):\n",
    "            fp += 1\n",
    "        if (pred_safety[i]==\"unsafe\") & (true_safety[i]==\"unsafe\"):\n",
    "            tn += 1\n",
    "        if (pred_safety[i]==\"unsafe\") & (true_safety[i]==\"safe\"):\n",
    "            fn += 1\n",
    "            \n",
    "    p = tp/(tp+fp)\n",
    "    r = tp/(tp+fn)\n",
    "    if p==0 and r==0:\n",
    "        f1score = 0\n",
    "    else: f1score = (2*p*r)/(p+r)\n",
    "        \n",
    "    return(tp,fp,tn,fn,p,r,f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allergens and True Safety Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_A = ['milk', 'cheese', 'soy', 'cream', 'eggs']\n",
    "person_B = ['peanuts', 'whey', 'tree nuts']\n",
    "person_C = ['Shrimp', 'Prawns', 'Lobster', 'Crab']\n",
    "person_D = ['wheat', 'barely', 'rye triticale']\n",
    "person_E = ['garlic', 'avocado', 'celery']\n",
    "\n",
    "allergens = [person_A, person_B, person_C, person_D, person_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean of presence of allergens\n",
    "top_bool_safety = [[[a in ing for a in person] for ing in a_tex_ings] for person in allergens] # for top ingredients \n",
    "bool_safety = [[[a in ing for a in person] for ing in ingredients] for person in allergens] # for all ingredients \n",
    "\n",
    "# true safety of each product\n",
    "top_true_safety = [[\"unsafe\" if sum(s)>0 else \"safe\" for s in person] for person in top_bool_safety]\n",
    "true_safety = [[\"unsafe\" if sum(s)>0 else \"safe\" for s in person] for person in bool_safety] \n",
    "\n",
    "print(len(top_true_safety[0]))\n",
    "print(len(true_safety[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual safety for persons \n",
    "print(\"top safe\", [sum([s == \"safe\" for s in person]) for person in top_true_safety], \"top unsafe\", [sum([s == \"unsafe\" for s in person]) for person in top_true_safety])\n",
    "print(\"all safe\", [sum([s == \"safe\" for s in person]) for person in true_safety], \"all unsafe\", [sum([s == \"unsafe\" for s in person]) for person in true_safety])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tex_ings = top_tex_ingredients[2] # top detected ingredients\n",
    "#tex_ingredients <- all detected ingredients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply perfect matching\n",
    "def safety_basic (allergens, ingredients): \n",
    "    if len(ingredients) == 0 or ingredients == ['']:\n",
    "        return \"Nothing detected. Please retake photo.\" \n",
    "    else: \n",
    "        ocr_safety = [a in ingredients for a in allergens] # creates boolean array of presence of allergen in ingredients\n",
    "        if sum(ocr_safety) > 0:\n",
    "            return \"unsafe\"\n",
    "        else: return \"safe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_basic_safety = [[safety_basic(person, ing) for ing in top_tex_ings] for person in allergens] # detected safety for top ingredients for each person\n",
    "basic_safety = [[safety_basic(person, ing) for ing in tex_ingredients] for person in allergens] # detected safety for all ingredients for each person\n",
    "\n",
    "print(\"--- %s seconds ---\" % ((time.time() - start_time)/(1210*5))) # 0.000003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(top_basic_safety[0]))\n",
    "print(len(basic_safety[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_basic_scores = [pred_stats(top_basic_safety[i], top_true_safety[i]) for i in range(5)] # scores for top ingredients for each person \n",
    "basic_scores = [pred_stats(basic_safety[i], true_safety[i]) for i in range(5)] # scores for all ingredients for each person \n",
    "\n",
    "print(len(top_basic_scores[0]))\n",
    "print(len(basic_scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safety_count (allergens, ingredients, sim):\n",
    "    if len(ingredients) == 0 or ingredients == ['']:\n",
    "        return \"Nothing detected. Please retake photo.\" \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='char')\n",
    "        words = allergens + ingredients # create list of allergens and ingredients \n",
    "        word_embeddings = vectorizer.fit_transform(words) # embeddings for allergens and ingredients \n",
    "        cos_sims = cosine_similarity(word_embeddings[:len(allergens)], word_embeddings[len(allergens):]) # cos sim of allergens and ingredients \n",
    "        counts = [sum(all_sims) for all_sims in cos_sims > sim] # counts of ingredients with a 'sim' greater cos sim to each allergen \n",
    "        if sum(counts) > 0:\n",
    "            return \"unsafe\" \n",
    "        else: return \"safe\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sims = [0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "count_safes = [dect_safety(safety_count, sim) for sim in count_sims]\n",
    "top_count_safety = [count_s[0] for count_s in count_safes] # detected safety for top ingredients for every cosine threshold\n",
    "count_safety = [count_s[1] for count_s in count_safes] # detected safety for all ingredients for every cosine threshold\n",
    "print(sum([count_s[2] for count_s in count_safes])/5) # 0.00230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(top_count_safety[0][0]))\n",
    "print(len(count_safety[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_count_scores = [[pred_stats(top_count_safety[s][i], top_true_safety[i]) for i in range(5)] for s in range(5)] # scores for top ingredients for each cosine and each person\n",
    "count_scores = [[pred_stats(count_safety[s][i], true_safety[i]) for i in range(5)] for s in range(5)] # scores for top ingredients for each cosine and each person\n",
    "\n",
    "print(len(top_count_scores[0][0]))\n",
    "print(len(count_scores[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chars2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safety_chars (allergens, ingredients, sim):\n",
    "    if len(ingredients) == 0 or ingredients == ['']:\n",
    "        return \"Nothing detected. Please retake photo.\"\n",
    "    else:\n",
    "        c2v_model = chars2vec.load_model('eng_50')\n",
    "        words = allergens + ingredients # create list of allergens and ingredients \n",
    "        word_embeddings = c2v_model.vectorize_words(words) # embeddings for allergens and ingredients \n",
    "        cos_sims = cosine_similarity(word_embeddings[:len(allergens)], word_embeddings[len(allergens):]) # cos sim of allergens and ingredients \n",
    "        counts = [sum(all_sims) for all_sims in cos_sims > sim] # counts of ingredients with a 'sim' greater cos sim to each allergen \n",
    "        if sum(counts) > 0:\n",
    "            return \"unsafe\"\n",
    "        else: return \"safe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_chars[person_A, top_tex_ings, 0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_sims = [0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "chars_safes = [dect_safety(safety_chars, sim) for sim in chars_sims]\n",
    "top_chars_safety = [chars_s[0] for chars_s in chars_safes] # detected safety for top ingredients for every cosine threshold\n",
    "chars_safety = [chars_s[1] for chars_s in chars_safes] # detected safety for top ingredients for every cosine threshold\n",
    "print(sum([chars_s[2] for chars_s in chars_safes])/5) # 0.000956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(top_chars_safety[0][0]))\n",
    "print(len(chars_safety[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chars_scores = [[pred_stats(top_chars_safety[s][i], top_true_safety[i]) for i in range(5)] for s in range(5)] # scores for top ingredients for each cosine and each person\n",
    "chars_scores = [[pred_stats(chars_safety[s][i], true_safety[i]) for i in range(5)] for s in range(5)] # scores for top ingredients for each cosine and each person\n",
    "\n",
    "print(len(top_chars_scores[0][0]))\n",
    "print(len(chars_scores[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different thresholds of the cosine similarity score, we can draw an ROC curve to determine the best threshold for the tradeoff we are willing to accept. We can also decide to use CountVectorizer or Chars2Vec based on which one has a greater AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity = recall =  tp/(tp+fn) : true safe detections out of all safe\n",
    "def tpr(tex_scores): \n",
    "    return [sum([scores[5] for scores in tex_scores[i]])/5 for i in range(5)]\n",
    "  \n",
    "# 1-specificity = false positive rate = fp/(fp+tn) : false safe detections out of all unsafe\n",
    "def fpr(tex_scores):\n",
    "    return [sum([scores[1]/(scores[1]+scores[2]) for scores in tex_scores[i] if (scores[1]+scores[2])!=0])/5 for i in range(5)]\n",
    "    \n",
    "# (0:tp, 1:fp, 2:tn, 3:fn, 4:p, 5:r, 6:f1score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\"\n",
    "\n",
    "def plotROC (tex_scores1, tex_scores2, tex_scores3):\n",
    "    \n",
    "    x1 = fpr(tex_scores1)\n",
    "    y1 = tpr(tex_scores1)\n",
    "\n",
    "    x2 = fpr(tex_scores2)\n",
    "    y2 = tpr(tex_scores2)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    \n",
    "    x3 = sum([scores[1]/(scores[1]+scores[2]) for scores in tex_scores3 if (scores[1]+scores[2])!=0])/5\n",
    "    y3 = sum([scores[5] for scores in tex_scores3])/5\n",
    "\n",
    "    plt.ylabel(\"True Positive Rate\", color=\"#447b72\", weight=\"bold\")\n",
    "    plt.yticks(fontsize=12, color=\"#234943\")\n",
    "    plt.xlabel(\"False Positive Rate\", color=\"#447b72\", weight=\"bold\")\n",
    "    plt.xticks(fontsize=12, color=\"#234943\")\n",
    "    #plt.title(\"All Images\")\n",
    "\n",
    "    plt.plot(x3, y3, 'Db', label='Basic Match', alpha=0.75)\n",
    "    plt.plot(x1, y1, 'c', label='CountVectorizer')\n",
    "    plt.plot(x2, y2, 'y', label='Chars2Vec')\n",
    "    plt.plot(x3, y3, 'Db')\n",
    "\n",
    "    legend = ax.legend(loc='upper left', edgecolor='#447b72', shadow=True, prop={'weight':'bold', 'size':10})\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor(\"white\")\n",
    "    \n",
    "    for text in legend.get_texts():\n",
    "        text.set_color(\"#447b72\")\n",
    "\n",
    "    verts1 = [(x1[0], 0), *zip(x1, y1), (x1[4], 0)]\n",
    "    poly1 = Polygon(verts1, facecolor='0.9', edgecolor='0.5', color=\"c\", alpha=0.1)\n",
    "    ax.add_patch(poly1)\n",
    "\n",
    "    verts2 = [(x2[0], 0), *zip(x2, y2), (x2[4], 0)]\n",
    "    poly2 = Polygon(verts2, facecolor='0.9', edgecolor='0.5', color=\"y\", alpha=0.1)\n",
    "    ax.add_patch(poly2)\n",
    "    \n",
    "    fig.set_facecolor('white')\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "#     for i, txt in enumerate(s):\n",
    "#         ax.annotate(txt, (x1[i], y1[i]+0.02), fontsize=10)\n",
    "#         ax.annotate(txt, (x2[i], y2[i]-0.02), fontsize=10)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plotROC(count_scores, chars_scores, basic_scores) # all ingredients \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plotROC(top_count_scores, top_chars_scores, top_basic_scores) # top ingredients \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
